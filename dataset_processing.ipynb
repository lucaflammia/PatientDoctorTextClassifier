{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "HF_USERNAME = \"LukeGPT88\"\n",
    "PROJECT_NAME = \"patient-doctor-text-classifier\"\n",
    "SUB_PROJECT_NAME = \"eng\"\n",
    "DATASET_NAME = f\"{HF_USERNAME}/{PROJECT_NAME}-{SUB_PROJECT_NAME}-dataset\"\n",
    "TASK = f\"{PROJECT_NAME}-{SUB_PROJECT_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "id2label = {0: \"PATIENT\", 1: \"DOCTOR\", 2: \"NEUTRAL\"}\n",
    "\n",
    "def mapping(idx):\n",
    "  label = id2label.get(idx)\n",
    "  return label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FOR OLD DATASET ONLY (HF repo patient-doctor-text-classifier-eng-dataset-old)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert short texts in neutral conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_train_text = list(map(mapping, dataset['train']['Label']))\n",
    "# label_val_text = list(map(mapping, dataset['validation']['Label']))\n",
    "# label_test_text = list(map(mapping, dataset['test']['Label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame({'Text': dataset['train']['Text'], 'Label': dataset['train']['Label'], 'Encoding': dataset['train']['Encoding'] })\n",
    "df_val = pd.DataFrame({'Text': dataset['validation']['Text'], 'Label': dataset['validation']['Label'], 'Encoding': dataset['validation']['Encoding'] })\n",
    "df_test = pd.DataFrame({'Text': dataset['test']['Text'], 'Label': dataset['test']['Label'], 'Encoding': dataset['test']['Encoding'] })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train.to_csv('df_train_old.csv')\n",
    "# df_val.to_csv('df_val_old.csv')\n",
    "# df_test.to_csv('df_test_old.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHORT_TEXT_LENGTH = 10\n",
    "df_train['Short Text'] = [True if len(text) < SHORT_TEXT_LENGTH else False for text in df_train['Text'].values]\n",
    "df_val['Short Text'] = [True if len(text) < SHORT_TEXT_LENGTH else False for text in df_val['Text'].values]\n",
    "df_test['Short Text'] = [True if len(text) < SHORT_TEXT_LENGTH else False for text in df_test['Text'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_val['Text'].loc[df_val['Short Text'] == True].to_csv('df_val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to convert short expressions to be classified as neutral conversions\n",
    "# matches all the patterns that are similar to the requested word\n",
    "import re\n",
    "\n",
    "texts = df_train['Text'].loc[ (df_train['Short Text'] == True) ]\n",
    "indexes = df_train['Text'].loc[ (df_train['Short Text'] == True) ].index\n",
    "\n",
    "words = ['uhm', 'ok', 'no', 'uh', 'sure', 'mmm']\n",
    "text_to_convert = []\n",
    "rows_to_convert = []\n",
    "for text, index in list(map(lambda x, y: (x, y), texts,indexes.values)):\n",
    "  for word in words:\n",
    "\n",
    "    regex_pattern = r\"\\W*\\w*\" + re.escape(word) + r'\\W*\\w*'\n",
    "\n",
    "    matches = re.findall(regex_pattern, text, re.IGNORECASE)\n",
    "    if len(matches) > 0 :\n",
    "      rows_to_convert.append(index)\n",
    "      text_to_convert.append([matches[0], index])\n",
    "print(len(rows_to_convert))\n",
    "print(len(list(set(rows_to_convert))))\n",
    "print(text_to_convert)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.iloc[list(set(rows_to_convert))][df_train['Label'].isin(['DOCTOR'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.iloc[list(set(rows_to_convert)), df_train.columns.get_loc('Label')] = 'NEUTRAL'\n",
    "df_train.iloc[list(set(rows_to_convert)), df_train.columns.get_loc('Encoding')] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to convert short expressions to be classified as neutral conversions\n",
    "# matches all the patterns that are similar to the requested word\n",
    "import re\n",
    "\n",
    "texts = df_val['Text'].loc[ (df_val['Short Text'] == True) ]\n",
    "indexes = df_val['Text'].loc[ (df_val['Short Text'] == True) ].index\n",
    "\n",
    "words = ['uhm', 'ok', 'no', 'uh', 'sure', 'mmm']\n",
    "text_to_convert = []\n",
    "rows_to_convert = []\n",
    "for text, index in list(map(lambda x, y: (x, y), texts,indexes.values)):\n",
    "  for word in words:\n",
    "\n",
    "    regex_pattern = r\"\\W*\\w*\" + re.escape(word) + r'\\W*\\w*'\n",
    "\n",
    "    matches = re.findall(regex_pattern, text, re.IGNORECASE)\n",
    "    if len(matches) > 0 :\n",
    "      rows_to_convert.append(index)\n",
    "      text_to_convert.append([matches[0], index])\n",
    "print(len(rows_to_convert))\n",
    "print(len(list(set(rows_to_convert))))\n",
    "print(text_to_convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val.iloc[list(set(rows_to_convert))][df_val['Label'].isin(['DOCTOR'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val.iloc[list(set(rows_to_convert)), df_val.columns.get_loc('Label')] = 'NEUTRAL'\n",
    "df_val.iloc[list(set(rows_to_convert)), df_val.columns.get_loc('Encoding')] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to convert short expressions to be classified as neutral conversions\n",
    "# matches all the patterns that are similar to the requested word\n",
    "import re\n",
    "\n",
    "texts = df_test['Text'].loc[ (df_test['Short Text'] == True) ]\n",
    "indexes = df_test['Text'].loc[ (df_test['Short Text'] == True) ].index\n",
    "\n",
    "words = ['uhm', 'ok', 'no', 'uh', 'sure', 'mmm']\n",
    "text_to_convert = []\n",
    "rows_to_convert = []\n",
    "for text, index in list(map(lambda x, y: (x, y), texts,indexes.values)):\n",
    "  for word in words:\n",
    "\n",
    "    regex_pattern = r\"\\W*\\w*\" + re.escape(word) + r'\\W*\\w*'\n",
    "\n",
    "    matches = re.findall(regex_pattern, text, re.IGNORECASE)\n",
    "    if len(matches) > 0 :\n",
    "      rows_to_convert.append(index)\n",
    "      text_to_convert.append([matches[0], index])\n",
    "print(len(rows_to_convert))\n",
    "print(len(list(set(rows_to_convert))))\n",
    "print(text_to_convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.iloc[list(set(rows_to_convert)), df_test.columns.get_loc('Label')] = 'NEUTRAL'\n",
    "df_test.iloc[list(set(rows_to_convert)), df_test.columns.get_loc('Encoding')] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Dataframes to CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('df_train.csv')\n",
    "df_val.to_csv('df_val.csv')\n",
    "df_test.to_csv('df_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Pandas Dataframe To The HF Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop('Short Text', axis=1, inplace=True)\n",
    "df_val.drop('Short Text', axis=1, inplace=True)\n",
    "df_test.drop('Short Text', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "new_ds = DatasetDict()\n",
    "\n",
    "new_ds['train'] = Dataset.from_pandas(df_train)\n",
    "new_ds['validation'] = Dataset.from_pandas(df_val)\n",
    "new_ds['test'] = Dataset.from_pandas(df_test)\n",
    "\n",
    "print(new_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HF Login & Dataset Push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ds.push_to_hub(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Size For Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "size_in_bytes = []\n",
    "\n",
    "for text in new_ds['train']['Text']:\n",
    "  size_in_bytes.append(sys.getsizeof(text) - sys.getsizeof(\"\"))\n",
    "\n",
    "print(f\"Total Size of text strings: {sum(size_in_bytes)} bytes\")\n",
    "print(f\"Average Size for each string: {sum(size_in_bytes)/len(new_ds['train']['Text'])} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the Classification Score Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\"text-classification\", model=f\"{HF_USERNAME}/{TASK}\", top_k=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "pred_score_list = []\n",
    "for text in dataset['test']['Text']:\n",
    "  res = classifier(text)\n",
    "  pred_score_list.append(res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['Classification Score'] = pred_score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Classification Score'] = [[{'label': 'PATIENT', 'score': float(0)}, {'label': 'DOCTOR', 'score': float(0)}, {'label': 'NEUTRAL', 'score': float(0)}]] * len(df_train.index)\n",
    "df_val['Classification Score'] = [[{'label': 'PATIENT', 'score': float(0)}, {'label': 'DOCTOR', 'score': float(0)}, {'label': 'NEUTRAL', 'score': float(0)}]] * len(df_val.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\"text-classification\", model=f\"{HF_USERNAME}/{TASK}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "pred_list = []\n",
    "for text in dataset['test']['Text']:\n",
    "  res = classifier(text)\n",
    "  pred_list.append(res[0]['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "true_list = list(map(mapping, dataset['test']['Encoding']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Plot a confusion matrix.\n",
    "# cm is the confusion matrix, names are the names of the classes.\n",
    "def plot_confusion_matrix(cm, names, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "  plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "  plt.title(title)\n",
    "  plt.colorbar()\n",
    "  tick_marks = np.arange(len(names))\n",
    "  plt.xticks(tick_marks, names, rotation=45)\n",
    "  plt.yticks(tick_marks, names)\n",
    "  plt.tight_layout()\n",
    "  plt.ylabel('True label')\n",
    "  plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(true_list, pred_list)\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Normalize the confusion matrix by row (i.e by the number of samples\n",
    "# in each class)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "print('Normalized Confusion Matrix')\n",
    "print(cm_normalized)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm_normalized, id2label.values(), title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "macro = precision_recall_fscore_support(true_list, pred_list, average='macro')\n",
    "micro = precision_recall_fscore_support(true_list, pred_list, average='micro')\n",
    "accuracy_score = accuracy_score(true_list, pred_list)\n",
    "labels_stats = precision_recall_fscore_support(true_list, pred_list, average=None)\n",
    "\n",
    "print(f'Macro : {macro}\\nMicro : {micro}\\nAccuracy Score : {accuracy_score}\\nLabels Stats : {labels_stats}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
